{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c547d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "from collections import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "747a4b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AdienceBenchmarkGenderAndAgeClassification', 'faces', 'fold_0_data.txt', 'fold_1_data.txt', 'fold_2_data.txt', 'fold_3_data.txt', 'fold_4_data.txt', '__MACOSX']\n"
     ]
    }
   ],
   "source": [
    "data_parent = 'C:/Users/user/Desktop/AdienceBenchmarkGenderAndAgeClassification/'\n",
    "print(os.listdir(data_parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ea154b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>original_image</th>\n",
       "      <th>face_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>dx</th>\n",
       "      <th>dy</th>\n",
       "      <th>tilt_ang</th>\n",
       "      <th>fiducial_yaw_angle</th>\n",
       "      <th>fiducial_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30601258@N03</td>\n",
       "      <td>10399646885_67c7d20df9_o.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>414</td>\n",
       "      <td>1086</td>\n",
       "      <td>1383</td>\n",
       "      <td>-115</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30601258@N03</td>\n",
       "      <td>10424815813_e94629b1ec_o.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>m</td>\n",
       "      <td>301</td>\n",
       "      <td>105</td>\n",
       "      <td>640</td>\n",
       "      <td>641</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30601258@N03</td>\n",
       "      <td>10437979845_5985be4b26_o.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>f</td>\n",
       "      <td>2395</td>\n",
       "      <td>876</td>\n",
       "      <td>771</td>\n",
       "      <td>771</td>\n",
       "      <td>175</td>\n",
       "      <td>-30</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30601258@N03</td>\n",
       "      <td>10437979845_5985be4b26_o.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>m</td>\n",
       "      <td>752</td>\n",
       "      <td>1255</td>\n",
       "      <td>484</td>\n",
       "      <td>485</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30601258@N03</td>\n",
       "      <td>11816644924_075c3d8d59_o.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>m</td>\n",
       "      <td>175</td>\n",
       "      <td>80</td>\n",
       "      <td>769</td>\n",
       "      <td>768</td>\n",
       "      <td>-75</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id                original_image  face_id       age gender     x  \\\n",
       "0  30601258@N03  10399646885_67c7d20df9_o.jpg        1  (25, 32)      f     0   \n",
       "1  30601258@N03  10424815813_e94629b1ec_o.jpg        2  (25, 32)      m   301   \n",
       "2  30601258@N03  10437979845_5985be4b26_o.jpg        1  (25, 32)      f  2395   \n",
       "3  30601258@N03  10437979845_5985be4b26_o.jpg        3  (25, 32)      m   752   \n",
       "4  30601258@N03  11816644924_075c3d8d59_o.jpg        2  (25, 32)      m   175   \n",
       "\n",
       "      y    dx    dy  tilt_ang  fiducial_yaw_angle  fiducial_score  \n",
       "0   414  1086  1383      -115                  30              17  \n",
       "1   105   640   641         0                   0              94  \n",
       "2   876   771   771       175                 -30              74  \n",
       "3  1255   484   485       180                   0              47  \n",
       "4    80   769   768       -75                   0              34  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_0 = pd.read_csv(os.path.join(data_parent, 'fold_0_data.txt'), sep='\\t')\n",
    "fold_1 = pd.read_csv(os.path.join(data_parent, 'fold_1_data.txt'),sep='\\t')\n",
    "fold_2 = pd.read_csv(os.path.join(data_parent, 'fold_2_data.txt'),sep='\\t')\n",
    "fold_3 = pd.read_csv(os.path.join(data_parent, 'fold_3_data.txt'),sep='\\t')\n",
    "fold_4 = pd.read_csv(os.path.join(data_parent, 'fold_4_data.txt'),sep='\\t')\n",
    "total_data = pd.concat([fold_0, fold_1, fold_2, fold_3, fold_4], ignore_index=True)\n",
    "total_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5d32b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25-32    5296\n",
       "38-43    2776\n",
       "0-2      2509\n",
       "8-13     2292\n",
       "4-6      2140\n",
       "15-20    1792\n",
       "48-53     916\n",
       "60+       901\n",
       "Name: age, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_mapping = [('(0, 2)', '0-2'), ('2', '0-2'), ('3', '0-2'), ('(4, 6)', '4-6'), ('(8, 12)', '8-13'), ('13', '8-13'), ('22', '15-20'), ('(8, 23)','15-20'), ('23', '25-32'), ('(15, 20)', '15-20'), ('(25, 32)', '25-32'), ('(27, 32)', '25-32'), ('32', '25-32'), ('34', '25-32'), ('29', '25-32'), ('(38, 42)', '38-43'), ('35', '38-43'), ('36', '38-43'), ('42', '48-53'), ('45', '38-43'), ('(38, 43)', '38-43'), ('(38, 42)', '38-43'), ('(38, 48)', '48-53'), ('46', '48-53'), ('(48, 53)', '48-53'), ('55', '48-53'), ('56', '48-53'), ('(60, 100)', '60+'), ('57', '60+'), ('58', '60+')]\n",
    "age_mapping_dict = {each[0]: each[1] for each in age_mapping}\n",
    "\n",
    "drop_labels = []\n",
    "for idx, each in enumerate(total_data.age):\n",
    "    if each == 'None':\n",
    "        drop_labels.append(idx)\n",
    "    else:\n",
    "        total_data.age.loc[idx] = age_mapping_dict[each]\n",
    "total_data = total_data.drop(labels=drop_labels, axis=0) #droped None values\n",
    "total_data.age.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e749af12",
   "metadata": {},
   "source": [
    "## 48세 미만의 data 12156개, 48세 이상의 data 1817개"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab2908f",
   "metadata": {},
   "source": [
    "## data 상당히 불균형하여 48세 미만 데이터 연령대별로 몇 개씩 랜덤으로 추출하여 균형 잡힌 데이터 생성"
   ]
  },
  {
   "attachments": {
    "graph.PNG": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "eaaae02e",
   "metadata": {},
   "source": [
    "![graph.PNG](attachment:graph.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e752b1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['25-32', '38-43', '4-6', '60+', '15-20', '48-53', '8-13', '0-2'],\n",
       "       dtype=object),\n",
       " 8,\n",
       " array(['f', 'm', 'u'], dtype=object))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data = total_data.dropna()\n",
    "total_data['full_path'] = total_data.apply(lambda x: os.path.join(data_parent, 'faces', str(x.user_id), 'coarse_tilt_aligned_face.' + str(x.face_id) + '.' + x.original_image), axis=1)\n",
    "total_data.age.unique(), len(total_data.age.unique()), total_data.gender.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef5f819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_map = {\n",
    "    '0-2'  :100,\n",
    "    '4-6'  :100,\n",
    "    '8-13' :2,\n",
    "    '15-20':3,\n",
    "    '25-32':4,\n",
    "    '38-43':5,\n",
    "    '48-53':1,\n",
    "    '60+'  :1\n",
    "}\n",
    "\n",
    "total_data.age=total_data.age.replace(age_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b482587",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_labels= total_data.age.values.tolist()\n",
    "train_paths = total_data.full_path.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5810715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slist = list(zip(train_paths, age_labels))\n",
    "slist[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5b3c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = []\n",
    "list2 = []\n",
    "list3 = []\n",
    "list4 = []\n",
    "list5 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f107d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(slist)):\n",
    "    if slist[i][1] == 2:\n",
    "        list2.append(slist[i])\n",
    "    elif slist[i][1] == 3:\n",
    "        list3.append(slist[i])\n",
    "    elif slist[i][1] == 4:\n",
    "        list4.append(slist[i])\n",
    "    elif slist[i][1] == 5:\n",
    "        list5.append(slist[i])\n",
    "    elif slist[i][1] == 1:\n",
    "        list1.append(slist[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "092447a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "list2 = random.sample(list2, 500)\n",
    "list3 = random.sample(list3, 500)\n",
    "list4 = random.sample(list4, 1000)\n",
    "list5 = random.sample(list5, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29ab8d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "back = list2+list3+list4+list5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b087bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "for i in range(len(back)):\n",
    "    vlist = list(back[i])\n",
    "    vlist[1] = 0\n",
    "    new_list.append(tuple(vlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fbc43ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = new_list+list1\n",
    "random.seed(1234)\n",
    "new_list = random.sample(new_list, len(new_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87551b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths, age_labels = zip(*new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "018c0cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data count: 3603 3603\n",
      "test data count: 1202 1202\n"
     ]
    }
   ],
   "source": [
    "train_split = 0.75\n",
    "train_sample = int(train_split * len(new_list))\n",
    "train_data = train_paths[:train_sample]\n",
    "test_data = train_paths[train_sample:]\n",
    "\n",
    "train_labels_age=age_labels[:train_sample]\n",
    "test_labels_age=age_labels[train_sample:]\n",
    "print(\"train data count:\", len(train_data), len(train_labels_age))\n",
    "print(\"test data count:\", len(test_data), len(test_labels_age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c389cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'file_path' : train_data, 'age_label' : train_labels_age})\n",
    "validation_df = pd.DataFrame({'file_path' : test_data, 'age_label' : test_labels_age})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7b981ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>age_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Users/user/Desktop/AdienceBenchmarkGenderAn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:/Users/user/Desktop/AdienceBenchmarkGenderAn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:/Users/user/Desktop/AdienceBenchmarkGenderAn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:/Users/user/Desktop/AdienceBenchmarkGenderAn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:/Users/user/Desktop/AdienceBenchmarkGenderAn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>C:/Users/user/Desktop/AdienceBenchmarkGenderAn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>C:/Users/user/Desktop/AdienceBenchmarkGenderAn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3600</th>\n",
       "      <td>C:/Users/user/Desktop/AdienceBenchmarkGenderAn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3601</th>\n",
       "      <td>C:/Users/user/Desktop/AdienceBenchmarkGenderAn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3602</th>\n",
       "      <td>C:/Users/user/Desktop/AdienceBenchmarkGenderAn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3603 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file_path  age_label\n",
       "0     C:/Users/user/Desktop/AdienceBenchmarkGenderAn...          1\n",
       "1     C:/Users/user/Desktop/AdienceBenchmarkGenderAn...          0\n",
       "2     C:/Users/user/Desktop/AdienceBenchmarkGenderAn...          0\n",
       "3     C:/Users/user/Desktop/AdienceBenchmarkGenderAn...          0\n",
       "4     C:/Users/user/Desktop/AdienceBenchmarkGenderAn...          1\n",
       "...                                                 ...        ...\n",
       "3598  C:/Users/user/Desktop/AdienceBenchmarkGenderAn...          0\n",
       "3599  C:/Users/user/Desktop/AdienceBenchmarkGenderAn...          0\n",
       "3600  C:/Users/user/Desktop/AdienceBenchmarkGenderAn...          1\n",
       "3601  C:/Users/user/Desktop/AdienceBenchmarkGenderAn...          0\n",
       "3602  C:/Users/user/Desktop/AdienceBenchmarkGenderAn...          0\n",
       "\n",
       "[3603 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd4984d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 1351, 0: 2252})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_df['age_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f45be944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>age_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Users/user/Desktop/AdienceBenchmarkGenderAn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:/Users/user/Desktop/AdienceBenchmarkGenderAn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:/Users/user/Desktop/AdienceBenchmarkGenderAn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:/Users/user/Desktop/AdienceBenchmarkGenderAn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:/Users/user/Desktop/AdienceBenchmarkGenderAn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>C:/Users/user/Desktop/AdienceBenchmarkGenderAn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>C:/Users/user/Desktop/AdienceBenchmarkGenderAn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>C:/Users/user/Desktop/AdienceBenchmarkGenderAn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>C:/Users/user/Desktop/AdienceBenchmarkGenderAn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>C:/Users/user/Desktop/AdienceBenchmarkGenderAn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1202 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file_path  age_label\n",
       "0     C:/Users/user/Desktop/AdienceBenchmarkGenderAn...          0\n",
       "1     C:/Users/user/Desktop/AdienceBenchmarkGenderAn...          0\n",
       "2     C:/Users/user/Desktop/AdienceBenchmarkGenderAn...          0\n",
       "3     C:/Users/user/Desktop/AdienceBenchmarkGenderAn...          0\n",
       "4     C:/Users/user/Desktop/AdienceBenchmarkGenderAn...          1\n",
       "...                                                 ...        ...\n",
       "1197  C:/Users/user/Desktop/AdienceBenchmarkGenderAn...          0\n",
       "1198  C:/Users/user/Desktop/AdienceBenchmarkGenderAn...          1\n",
       "1199  C:/Users/user/Desktop/AdienceBenchmarkGenderAn...          0\n",
       "1200  C:/Users/user/Desktop/AdienceBenchmarkGenderAn...          0\n",
       "1201  C:/Users/user/Desktop/AdienceBenchmarkGenderAn...          1\n",
       "\n",
       "[1202 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42f21dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 748, 1: 454})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(validation_df['age_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a351ebd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file_path    C:/Users/user/Desktop/AdienceBenchmarkGenderAn...\n",
       "age_label                                                    0\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03792d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df.iloc[1][\"age_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72233f75",
   "metadata": {},
   "source": [
    "## Dataset class 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ed1beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_proc(index, df):\n",
    "    \n",
    "    img = cv2.imread(df.iloc[index]['file_path'], cv2.IMREAD_UNCHANGED)\n",
    "    resized_img = cv2.resize(img, (128,128), interpolation = cv2.INTER_AREA)\n",
    "    transform = transforms.ToTensor()\n",
    "    tensor = transform(resized_img)\n",
    "    \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c60c57ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class img_dataset(Dataset):\n",
    "    def __init__(self, data_frame):\n",
    "        self.data_frame = data_frame\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        y_data = self.data_frame.iloc[index]['age_label']\n",
    "        x_data = img_proc(index, self.data_frame)\n",
    "        return x_data, y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82cd5a3",
   "metadata": {},
   "source": [
    "## Dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13864bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = img_dataset(train_df)\n",
    "test_dataset = img_dataset(validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc1fe1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "                        dataset=train_dataset,\n",
    "                        batch_size = 16,\n",
    "                        shuffle=True,\n",
    "                        drop_last=True,\n",
    "                        num_workers = 0)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "                        dataset=test_dataset,\n",
    "                        batch_size = 16,\n",
    "                        shuffle=True,\n",
    "                        drop_last=True,\n",
    "                        num_workers = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33251ac",
   "metadata": {},
   "source": [
    "## Model 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "601fb561",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_1 = self._con_dw_sep(3, 16)\n",
    "        self.conv_2 = self._con_dw_sep(16, 32)\n",
    "        self.conv_3 = self._con_dw_sep(32, 64)\n",
    "        \n",
    "        self.fc1 = nn.Linear(10816, 512)\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def _con_dw_sep(self, C_in, C_out):\n",
    "        conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(C_in, C_in, kernel_size = 4, groups=C_in),\n",
    "            nn.Conv2d(C_in, C_out , kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        return conv_layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv_1(x)\n",
    "        out = self.conv_2(out)\n",
    "        out = self.conv_3(out)\n",
    "    \n",
    "        out = out.view(-1, 10816)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        out = out.squeeze()\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out.float()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebf89f2",
   "metadata": {},
   "source": [
    "## Model 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9bd69653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efc9e3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7ede789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv_1): Sequential(\n",
       "    (0): Conv2d(3, 3, kernel_size=(4, 4), stride=(1, 1), groups=3)\n",
       "    (1): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_2): Sequential(\n",
       "    (0): Conv2d(16, 16, kernel_size=(4, 4), stride=(1, 1), groups=16)\n",
       "    (1): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_3): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(4, 4), stride=(1, 1), groups=32)\n",
       "    (1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc1): Linear(in_features=10816, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2dcd55cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epoch = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9bbb29d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946ab877",
   "metadata": {},
   "source": [
    "### model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ca79db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./model/age_predict_cnn_sep'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = './model/age_predict_cnn_sep'\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e83dbf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.681788  [    0/ 3603]\n",
      "loss: 0.708410  [  800/ 3603]\n",
      "loss: 0.718020  [ 1600/ 3603]\n",
      "loss: 0.707983  [ 2400/ 3603]\n",
      "loss: 0.603502  [ 3200/ 3603]\n",
      "0.6716460484928555\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.663843  [    0/ 3603]\n",
      "loss: 0.691935  [  800/ 3603]\n",
      "loss: 0.531021  [ 1600/ 3603]\n",
      "loss: 0.736168  [ 2400/ 3603]\n",
      "loss: 0.651742  [ 3200/ 3603]\n",
      "0.6659776865111458\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.651972  [    0/ 3603]\n"
     ]
    }
   ],
   "source": [
    "train_epochs_loss = []\n",
    "train_batch_loss = []\n",
    "\n",
    "model.train()\n",
    "size = len(train_loader.dataset)\n",
    "for i in range(0, num_epoch):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    for batch ,(image, label) in enumerate(train_loader):\n",
    "        x = image.to(device)\n",
    "        y= label.float().to(device)\n",
    "        \n",
    "        output = model(x)\n",
    "        \n",
    "        loss = loss_func(output,y)\n",
    "        train_batch_loss.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            loss, current = loss.item(), batch * len(x)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "    print(np.mean(train_batch_loss))\n",
    "    print(\"\\n\")\n",
    "    train_epochs_loss.append(np.mean(train_batch_loss))\n",
    "    train_batch_loss = []\n",
    "    \n",
    "torch.save(model.state_dict(), save_path+'/age_cnn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2755f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_epochs_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2575d7b0",
   "metadata": {},
   "source": [
    "## test 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59845fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path+'/age_cnn.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff4c5d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "size = len(test_loader.dataset)\n",
    "model.eval()\n",
    "\n",
    "test_loss = []\n",
    "total_pred = []\n",
    "total_y = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch, (x, y) in enumerate(test_loader):\n",
    "        x, y = x.to(device), y.float().to(device)\n",
    "        \n",
    "        pred = model(x)\n",
    "        loss = loss_func(pred, y)\n",
    "        test_loss.append(loss.item())\n",
    "        \n",
    "        tmp_pred = pred.cpu().detach().numpy()\n",
    "        tmp_y = y.cpu().detach().numpy()\n",
    "        corr = np.corrcoef(tmp_pred, tmp_y)[1][0]\n",
    "        \n",
    "        total_pred += pred.cpu().numpy().tolist()\n",
    "        total_y += y.cpu().numpy().tolist()\n",
    "        \n",
    "        if batch % 5 == 0:\n",
    "            loss, current = loss.item(), batch * len(x)\n",
    "            print(f\"loss: {loss:>7f} \\t corr: {corr:>7f} \\t [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "loss = np.mean((np.array(total_pred)-np.array(total_y))*(np.array(total_pred)-np.array(total_y)))\n",
    "corr = np.corrcoef(np.array(total_pred), np.array(total_y))[1][0]\n",
    "\n",
    "print('\\n최종 loss & corr')\n",
    "print(f\"loss: {loss:>7f} \\t corr: {corr:>7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcecb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "up_list = []\n",
    "for i in range(len(total_pred)):\n",
    "    aa = total_pred[i]\n",
    "    if aa >= 0.7:\n",
    "        up_list.append(1.0)\n",
    "    else:\n",
    "        up_list.append(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8b83f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = len(total_pred) \n",
    "\n",
    "for i in range(total):\n",
    "    if total_y[i] == up_list[i]:\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbda447",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0685da",
   "metadata": {},
   "source": [
    "## 실제 이미지 가지고 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d197f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "young_man = 'C:/Users/user/Desktop/young_man.jpg'\n",
    "young_woman = 'C:/Users/user/Desktop/young_woman.jpg'\n",
    "old_man = 'C:/Users/user/Desktop/old_man.jpg'\n",
    "old_woman = 'C:/Users/user/Desktop/old_woman.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a94bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "tf = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3838b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = PIL.Image.open(young_man)\n",
    "img_t = tf(img)\n",
    "img_t = img_t.permute(1,2,0)\n",
    "plt.imshow(img_t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf0461",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = PIL.Image.open(young_woman)\n",
    "img_t = tf(img)\n",
    "img_t = img_t.permute(1,2,0)\n",
    "plt.imshow(img_t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345f6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = PIL.Image.open(old_man)\n",
    "img_t = tf(img)\n",
    "img_t = img_t.permute(1,2,0)\n",
    "plt.imshow(img_t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addd23c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = PIL.Image.open(old_woman)\n",
    "img_t = tf(img)\n",
    "img_t = img_t.permute(1,2,0)\n",
    "plt.imshow(img_t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88646c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_proc_for_test(path):\n",
    "    \n",
    "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    resized_img = cv2.resize(img, (128,128), interpolation = cv2.INTER_AREA)\n",
    "    transform = transforms.ToTensor()\n",
    "    tensor = transform(resized_img)\n",
    "    \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad57f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "young_man_tensor = img_proc_for_test(young_man).to(device)\n",
    "young_woman_tensor = img_proc_for_test(young_woman).to(device)\n",
    "old_man_tensor = img_proc_for_test(old_man).to(device)\n",
    "old_woman_tensor = img_proc_for_test(old_woman).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30accce",
   "metadata": {},
   "outputs": [],
   "source": [
    "young_man_out = model(young_man_tensor)\n",
    "young_woman_out = model(young_woman_tensor)\n",
    "old_man_out = model(old_man_tensor)\n",
    "old_woman_out = model(old_woman_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84be050",
   "metadata": {},
   "outputs": [],
   "source": [
    "young_man_out.item() # 0에 가까울스록 good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ad63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "young_woman_out.item() # 0에 가까울스록 good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2250a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_man_out.item()  # 1에 가까울스록 good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd4ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_woman_out.item() # 1에 가까울스록 good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1359c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_man2 = 'C:/Users/user/Desktop/oldman2.PNG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e49cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = PIL.Image.open(old_man2)\n",
    "img_t = tf(img)\n",
    "img_t = img_t.permute(1,2,0)\n",
    "plt.imshow(img_t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2528b113",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_man2_tensor = img_proc_for_test(old_man).to(device)\n",
    "old_man2_out = model(old_man2_tensor)\n",
    "old_man2_out.item()  # 1에 가까울스록 good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8faad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "\n",
    "# img = cv2.imread(old_man, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "# print('Original Dimensions : ',img.shape)\n",
    "\n",
    "# resized_img = cv2.resize(img, (128,128), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "# print('Resized Dimensions : ',resized_img.shape)\n",
    "\n",
    "# cv2.imshow(\"Resized image\", resized_img)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
